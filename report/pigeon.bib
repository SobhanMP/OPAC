
@article{abellAfricaPigeonTransfers,
  title = {In {{Africa}}, {{A Pigeon Transfers Data Faster Than The Internet}}},
  author = {Abell, John C.},
  journaltitle = {Wired},
  issn = {1059-1028},
  url = {https://www.wired.com/2009/09/in-africa-a-pigeon-transfers-data-faster-than-the-internet/},
  urldate = {2022-04-16},
  abstract = {JOHANNESBURG (Reuters) — A South African information technology company on Wednesday proved it was faster for them to transmit data with a carrier pigeon than to send it using Telkom, the country’s leading internet service provider. Internet speed and connectivity in Africa’s largest economy are poor because of a bandwidth shortage. It is also expensive. […]},
  entrysubtype = {magazine},
  langid = {american},
  keywords = {broadband},
  file = {/home/eta/Zotero/storage/NCJP4LKI/in-africa-a-pigeon-transfers-data-faster-than-the-internet.html}
}

@unpublished{amosDifferentiableMPCEndtoend2019,
  title = {Differentiable {{MPC}} for {{End-to-end Planning}} and {{Control}}},
  author = {Amos, Brandon and Rodriguez, Ivan Dario Jimenez and Sacks, Jacob and Boots, Byron and Kolter, J. Zico},
  date = {2019-10-14},
  eprint = {1810.13400},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1810.13400},
  urldate = {2022-02-20},
  abstract = {We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning in continuous state and action spaces. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/eta/Zotero/storage/GSXYCSEL/Amos et al. - 2019 - Differentiable MPC for End-to-end Planning and Con.pdf;/home/eta/Zotero/storage/HFREGYIB/1810.html}
}

@article{amosOptNetDifferentiableOptimization2017,
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  shorttitle = {{{OptNet}}},
  author = {Amos, Brandon and Kolter, J. Zico},
  date = {2017-03-01},
  url = {https://arxiv.org/abs/1703.00443v5},
  urldate = {2022-02-19},
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures.},
  langid = {english},
  file = {/home/eta/Zotero/storage/BW5B3YY4/Amos and Kolter - 2017 - OptNet Differentiable Optimization as a Layer in .pdf;/home/eta/Zotero/storage/3MCIFFS8/1703.html}
}

@unpublished{amosOptNetDifferentiableOptimization2021,
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  shorttitle = {{{OptNet}}},
  author = {Amos, Brandon and Kolter, J. Zico},
  date = {2021-12-02},
  eprint = {1703.00443},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/1703.00443},
  urldate = {2022-02-20},
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. We explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPU-based batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, the method is learns to play mini-Sudoku (4x4) given just input and output games, with no a-priori information about the rules of the game; this highlights the ability of OptNet to learn hard constraints better than other neural architectures.},
  archiveprefix = {arXiv},
  version = {4},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/eta/Zotero/storage/W2S2JGBA/Amos and Kolter - 2021 - OptNet Differentiable Optimization as a Layer in .pdf;/home/eta/Zotero/storage/722H3LTJ/1703.html}
}

@article{benamorChoiceExplicitStabilizing2009,
  title = {On the Choice of Explicit Stabilizing Terms in Column Generation},
  author = {Ben Amor, Hatem M. T. and Desrosiers, Jacques and Frangioni, Antonio},
  date = {2009-03-28},
  journaltitle = {Discrete Applied Mathematics},
  shortjournal = {Discrete Applied Mathematics},
  series = {Reformulation {{Techniques}} and {{Mathematical Programming}}},
  volume = {157},
  number = {6},
  pages = {1167--1184},
  issn = {0166-218X},
  doi = {10.1016/j.dam.2008.06.021},
  url = {https://www.sciencedirect.com/science/article/pii/S0166218X08002916},
  urldate = {2022-04-07},
  abstract = {Column generation algorithms are instrumental in many areas of applied optimization, where linear programs with an enormous number of columns need to be solved. Although successfully employed in many applications, these approaches suffer from well-known instability issues that somewhat limit their efficiency. Building on the theory developed for nondifferentiable optimization algorithms, a large class of stabilized column generation algorithms can be defined which avoid the instability issues by using an explicit stabilizing term in the dual; this amounts at considering a (generalized) augmented Lagrangian of the primal master problem. Since the theory allows for a great degree of flexibility in the choice and in the management of the stabilizing term, one can use piecewise-linear or quadratic functions that can be efficiently dealt with using off-the-shelf solvers. The practical effectiveness of this approach is demonstrated by extensive computational experiments on large-scale Vehicle and Crew Scheduling problems. Also, the results of a detailed computational study on the impact of the different choices in the stabilization term (shape of the function, parameters), and their relationships with the quality of the initial dual estimates, on the overall effectiveness of the approach are reported, providing practical guidelines for selecting the most appropriate variant in different situations.},
  langid = {english},
  keywords = {Bundle methods,Column generation,Proximal point methods,Vehicle and crew scheduling problems},
  file = {/home/eta/Zotero/storage/U3RD8M7F/S0166218X08002916.html}
}

@article{benamorChoiceExplicitStabilizing2009a,
  title = {On the Choice of Explicit Stabilizing Terms in Column Generation},
  author = {Ben Amor, Hatem M. T. and Desrosiers, Jacques and Frangioni, Antonio},
  date = {2009-03-28},
  journaltitle = {Discrete Applied Mathematics},
  shortjournal = {Discrete Applied Mathematics},
  series = {Reformulation {{Techniques}} and {{Mathematical Programming}}},
  volume = {157},
  number = {6},
  pages = {1167--1184},
  issn = {0166-218X},
  doi = {10.1016/j.dam.2008.06.021},
  url = {https://www.sciencedirect.com/science/article/pii/S0166218X08002916},
  urldate = {2022-04-07},
  abstract = {Column generation algorithms are instrumental in many areas of applied optimization, where linear programs with an enormous number of columns need to be solved. Although successfully employed in many applications, these approaches suffer from well-known instability issues that somewhat limit their efficiency. Building on the theory developed for nondifferentiable optimization algorithms, a large class of stabilized column generation algorithms can be defined which avoid the instability issues by using an explicit stabilizing term in the dual; this amounts at considering a (generalized) augmented Lagrangian of the primal master problem. Since the theory allows for a great degree of flexibility in the choice and in the management of the stabilizing term, one can use piecewise-linear or quadratic functions that can be efficiently dealt with using off-the-shelf solvers. The practical effectiveness of this approach is demonstrated by extensive computational experiments on large-scale Vehicle and Crew Scheduling problems. Also, the results of a detailed computational study on the impact of the different choices in the stabilization term (shape of the function, parameters), and their relationships with the quality of the initial dual estimates, on the overall effectiveness of the approach are reported, providing practical guidelines for selecting the most appropriate variant in different situations.},
  langid = {english},
  keywords = {Bundle methods,Column generation,Proximal point methods,Vehicle and crew scheduling problems}
}

@article{bendersPartitioningProceduresSolving1962,
  title = {Partitioning Procedures for Solving Mixed-Variables Programming Problems},
  author = {Benders, J. F.},
  date = {1962-12-01},
  journaltitle = {Numerische Mathematik},
  shortjournal = {Numer. Math.},
  volume = {4},
  number = {1},
  pages = {238--252},
  issn = {0945-3245},
  doi = {10.1007/BF01386316},
  url = {https://doi.org/10.1007/BF01386316},
  urldate = {2022-04-24},
  langid = {english},
  keywords = {Mathematical Method,Partitioning Procedure,Programming Problem},
  file = {/home/eta/Zotero/storage/HBZCIC9M/Benders - 1962 - Partitioning procedures for solving mixed-variable.pdf}
}

@article{bengioUsingFinancialTraining1997,
  title = {Using a Financial Training Criterion Rather than a Prediction Criterion},
  author = {Bengio, Yoshua},
  date = {1997},
  journaltitle = {International journal of neural systems},
  volume = {8},
  number = {04},
  pages = {433--443},
  publisher = {{World Scientific}},
  file = {/home/eta/Zotero/storage/74RLK7EL/Bengio - 1997 - Using a financial training criterion rather than a.pdf}
}

@article{dantzigDecompositionPrincipleLinear1960,
  title = {Decomposition {{Principle}} for {{Linear Programs}}},
  author = {Dantzig, George B. and Wolfe, Philip},
  date = {1960-02},
  journaltitle = {Operations Research},
  volume = {8},
  number = {1},
  pages = {101--111},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  doi = {10.1287/opre.8.1.101},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.8.1.101},
  urldate = {2022-04-24},
  abstract = {A technique is presented for the decomposition of a linear program that permits the problem to be solved by alternate solutions of linear sub-programs representing its several parts and a coordinating program that is obtained from the parts by linear transformations. The coordinating program generates at each cycle new objective forms for each part, and each part generates in turn (from its optimal basic feasible solutions) new activities (columns) for the interconnecting program. Viewed as an instance of a “generalized programming problem” whose columns are drawn freely from given convex sets, such a problem can be studied by an appropriate generalization of the duality theorem for linear programming, which permits a sharp distinction to be made between those constraints that pertain only to a part of the problem and those that connect its parts. This leads to a generalization of the Simplex Algorithm, for which the decomposition procedure becomes a special case. Besides holding promise for the efficient computation of large-scale systems, the principle yields a certain rationale for the “decentralized decision process” in the theory of the firm. Formally the prices generated by the coordinating program cause the manager of each part to look for a “pure” sub-program analogue of pure strategy in game theory, which he proposes to the coordinator as best he can do. The coordinator finds the optimum “mix” of pure sub-programs (using new proposals and earlier ones) consistent with over-all demands and supply, and thereby generates new prices that again generates new proposals by each of the parts, etc. The iterative process is finite.}
}

@article{demiroviDynamicProgrammingPredict2020,
  title = {Dynamic {{Programming}} for {{Predict}}+{{Optimise}}},
  author = {Demirovi?, Emir and Stuckey, Peter J. and Guns, Tias and Bailey, James and Leckie, Christopher and Ramamohanarao, Kotagiri and Chan, Jeffrey},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {02},
  pages = {1444--1451},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i02.5502},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/5502},
  urldate = {2022-02-19},
  abstract = {We study the predict+optimise problem, where machine learning and combinatorial optimisation must interact to achieve a common goal. These problems are important when optimisation needs to be performed on input parameters that are not fully observed but must instead be estimated using machine learning. We provide a novel learning technique for predict+optimise to directly reason about the underlying combinatorial optimisation problem, offering a meaningful integration of machine learning and optimisation. This is done by representing the combinatorial problem as a piecewise linear function parameterised by the coefficients of the learning model and then iteratively performing coordinate descent on the learning coefficients. Our approach is applicable to linear learning functions and any optimisation problem solvable by dynamic programming. We illustrate the effectiveness of our approach on benchmarks from the literature.},
  issue = {02},
  langid = {english},
  file = {/home/eta/Zotero/storage/HJR2FUSU/Demirovi et al. - 2020 - Dynamic Programming for Predict+Optimise.pdf}
}

@article{dijkstraNoteTwoProblems1959,
  title = {A Note on Two Problems in Connexion with Graphs},
  author = {Dijkstra, E. W.},
  date = {1959-12-01},
  journaltitle = {Numerische Mathematik},
  shortjournal = {Numer. Math.},
  volume = {1},
  number = {1},
  pages = {269--271},
  issn = {0945-3245},
  doi = {10.1007/BF01386390},
  url = {https://doi.org/10.1007/BF01386390},
  urldate = {2022-04-24},
  langid = {english},
  keywords = {Mathematical Method},
  file = {/home/eta/Zotero/storage/LI4H28D2/Dijkstra - 1959 - A note on two problems in connexion with graphs.pdf}
}

@online{Doi101016,
  title = {Doi:10.1016/j.Trb.2004.06.004 | {{Elsevier Enhanced Reader}}},
  shorttitle = {Doi},
  doi = {10.1016/j.trb.2004.06.004},
  url = {https://reader.elsevier.com/reader/sd/pii/S0191261504000992?token=5084F5369B2802F64D646C76182E75CE6A14C1F47FEE276F09A4316E332241F2CB0E9D32C4EA54FD703513A369A54C0F&originRegion=us-east-1&originCreation=20220327043847},
  urldate = {2022-03-27},
  langid = {english},
  file = {/home/eta/Zotero/storage/997R2AIK/S0191261504000992.html}
}

@article{elmachtoubSmartPredictThen2017,
  title = {Smart "{{Predict}}, Then {{Optimize}}"},
  author = {Elmachtoub, Adam N. and Grigas, Paul},
  date = {2017-10-22},
  url = {https://arxiv.org/abs/1710.08005v5},
  urldate = {2022-02-19},
  abstract = {Many real-world analytics problems involve two significant challenges: prediction and optimization. Due to the typically complex nature of each challenge, the standard paradigm is predict-then-optimize. By and large, machine learning tools are intended to minimize prediction error and do not account for how the predictions will be used in the downstream optimization problem. In contrast, we propose a new and very general framework, called Smart "Predict, then Optimize" (SPO), which directly leverages the optimization problem structure, i.e., its objective and constraints, for designing better prediction models. A key component of our framework is the SPO loss function which measures the decision error induced by a prediction. Training a prediction model with respect to the SPO loss is computationally challenging, and thus we derive, using duality theory, a convex surrogate loss function which we call the SPO+ loss. Most importantly, we prove that the SPO+ loss is statistically consistent with respect to the SPO loss under mild conditions. Our SPO+ loss function can tractably handle any polyhedral, convex, or even mixed-integer optimization problem with a linear objective. Numerical experiments on shortest path and portfolio optimization problems show that the SPO framework can lead to significant improvement under the predict-then-optimize paradigm, in particular when the prediction model being trained is misspecified. We find that linear models trained using SPO+ loss tend to dominate random forest algorithms, even when the ground truth is highly nonlinear.},
  langid = {english},
  file = {/home/eta/Zotero/storage/9DQN7TF8/Elmachtoub and Grigas - 2017 - Smart Predict, then Optimize.pdf}
}

@online{ElsevierEnhancedReader,
  title = {Elsevier {{Enhanced Reader}}},
  doi = {10.1016/j.orl.2009.02.003},
  url = {https://reader.elsevier.com/reader/sd/pii/S0167637709000339?token=D9D6C2D32CDC77CB4BCF79DAF9142110DC803431F09F8C4A7470F34D64C7B236986B3756B6CDF71435E4BFD7856F167D&originRegion=us-east-1&originCreation=20220327034159},
  urldate = {2022-03-27},
  langid = {english},
  file = {/home/eta/Zotero/storage/B9ZQM25H/S0167637709000339.html}
}

@online{ElsevierEnhancedReadera,
  title = {Elsevier {{Enhanced Reader}}},
  doi = {10.1016/j.orl.2009.02.003},
  url = {https://reader.elsevier.com/reader/sd/pii/S0167637709000339?token=D9D6C2D32CDC77CB4BCF79DAF9142110DC803431F09F8C4A7470F34D64C7B236986B3756B6CDF71435E4BFD7856F167D&originRegion=us-east-1&originCreation=20220327034159},
  urldate = {2022-03-27},
  langid = {english}
}

@inproceedings{ferberMIPaaLMixedInteger2020,
  title = {{{MIPaaL}}: {{Mixed Integer Program}} as a {{Layer}}},
  shorttitle = {{{MIPaaL}}},
  booktitle = {{{AAAI}}},
  author = {Ferber, Aaron and Wilder, B. and Dilkina, B. and Tambe, Milind},
  date = {2020},
  doi = {10.1609/AAAI.V34I02.5509},
  abstract = {This work enables decision-focused learning for the broad class of problems that can be encoded as a Mixed Integer Linear Program (MIP), hence supporting arbitrary linear constraints over discrete and continuous variables. Machine learning components commonly appear in larger decision-making pipelines; however, the model training process typically focuses only on a loss that measures average accuracy between predicted values and ground truth values. Decision-focused learning explicitly integrates the downstream decision problem when training the predictive model, in order to optimize the quality of decisions induced by the predictions. It has been successfully applied to several limited combinatorial problem classes, such as those that can be expressed as linear programs (LP), and submodular optimization. However, these previous applications have uniformly focused on problems with simple constraints. Here, we enable decision-focused learning for the broad class of problems that can be encoded as a mixed integer linear program (MIP), hence supporting arbitrary linear constraints over discrete and continuous variables. We show how to differentiate through a MIP by employing a cutting planes solution approach, an algorithm that iteratively tightens the continuous relaxation by adding constraints removing fractional solutions. We evaluate our new end-to-end approach on several real world domains and show that it outperforms the standard two phase approaches that treat prediction and optimization separately, as well as a baseline approach of simply applying decision-focused learning to the LP relaxation of the MIP. Lastly, we demonstrate generalization performance in several transfer learning tasks.},
  file = {/home/eta/Zotero/storage/K2FVNYJL/Ferber et al. - 2020 - MIPaaL Mixed Integer Program as a Layer.pdf}
}

@article{fordMaximalFlowNetwork1956,
  title = {Maximal {{Flow Through}} a {{Network}}},
  author = {Ford, L. R. and Fulkerson, D. R.},
  year = {1956/ed},
  journaltitle = {Canadian Journal of Mathematics},
  volume = {8},
  pages = {399--404},
  publisher = {{Cambridge University Press}},
  issn = {0008-414X, 1496-4279},
  doi = {10.4153/CJM-1956-045-5},
  url = {https://www.cambridge.org/core/journals/canadian-journal-of-mathematics/article/maximal-flow-through-a-network/5D6E55D3B06C4F7B1043BC1D82D40764},
  urldate = {2022-04-24},
  abstract = {Introduction. The problem discussed in this paper was formulated by T. Harris as follows:             “Consider a rail network connecting two cities by way of a number of intermediate cities, where each link of the network has a number assigned to it representing its capacity. Assuming a steady state condition, find a maximal flow from one given city to the other.”},
  langid = {english},
  file = {/home/eta/Zotero/storage/Q945IHJB/Ford and Fulkerson - 1956 - Maximal Flow Through a Network.pdf;/home/eta/Zotero/storage/GI5HW8WU/5D6E55D3B06C4F7B1043BC1D82D40764.html}
}

@article{gaoSolutionAlgorithmBilevel2005,
  title = {Solution Algorithm for the Bi-Level Discrete Network Design Problem},
  author = {Gao, Ziyou and Wu, Jianjun and Sun, Huijun},
  date = {2005-07-01},
  journaltitle = {Transportation Research Part B: Methodological},
  shortjournal = {Transportation Research Part B: Methodological},
  volume = {39},
  number = {6},
  pages = {479--495},
  issn = {0191-2615},
  doi = {10.1016/j.trb.2004.06.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0191261504000992},
  urldate = {2022-03-27},
  abstract = {The discrete network design problem deals with the selection of link additions to an existing road network, with given demand from each origin to each destination. The objective is to make an optimal investment decision in order to minimize the total travel cost in the network, while accounting for the route choice behaviors of network users. Because of the computational difficulties experienced with the solution algorithm of nonlinear bi-level mixed integer programming with a large number of 0–1 variables, the discrete network design problem has been recognized as one of the most difficult yet challenging problems in transport. In this paper, at first a traditional bi-level programming model for the discrete network design problem is introduced, and then a new solution algorithm is proposed by using the support function concept to express the relationship between improvement flows and the new additional links in the existing urban network. Finally, the applications of the new algorithm are illustrated with two numerical examples. Numerical results indicate that the proposed algorithm would be efficient in practice.},
  langid = {english},
  keywords = {Bi-level programming,Discrete network design problem,Solution algorithm,Support function},
  file = {/home/eta/Zotero/storage/X879D38A/Gao et al. - 2005 - Solution algorithm for the bi-level discrete netwo.pdf}
}

@article{geoffrionGeneralizedBendersDecomposition1972,
  title = {Generalized {{Benders}} Decomposition},
  author = {Geoffrion, A. M.},
  date = {1972-10-01},
  journaltitle = {Journal of Optimization Theory and Applications},
  shortjournal = {J Optim Theory Appl},
  volume = {10},
  number = {4},
  pages = {237--260},
  issn = {1573-2878},
  doi = {10.1007/BF00934810},
  url = {https://doi.org/10.1007/BF00934810},
  urldate = {2022-03-27},
  abstract = {J. F. Benders devised a clever approach for exploiting the structure of mathematical programming problems withcomplicating variables (variables which, when temporarily fixed, render the remaining optimization problem considerably more tractable). For the class of problems specifically considered by Benders, fixing the values of the complicating variables reduces the given problem to an ordinary linear program, parameterized, of course, by the value of the complicating variables vector. The algorithm he proposed for finding the optimal value of this vector employs a cutting-plane approach for building up adequate representations of (i) the extremal value of the linear program as a function of the parameterizing vector and (ii) the set of values of the parameterizing vector for which the linear program is feasible. Linear programming duality theory was employed to derive the natural families ofcuts characterizing these representations, and the parameterized linear program itself is used to generate what are usuallydeepest cuts for building up the representations.},
  langid = {english},
  file = {/home/eta/Zotero/storage/YPLSJIUT/Geoffrion - 1972 - Generalized Benders decomposition.pdf}
}

@online{IFT6756Class,
  title = {{{IFT-}} 6756 | {{Class Profile}} | {{Piazza}}},
  url = {https://piazza.com/umontreal.ca/spring2022/ift6756/resources},
  urldate = {2022-03-28},
  file = {/home/eta/Zotero/storage/M5ZFBQRD/resources.html}
}

@online{IntegerProgrammingFormulation,
  title = {Integer {{Programming Formulation}} of {{Traveling Salesman Problems}} | {{Journal}} of the {{ACM}}},
  url = {https://dl.acm.org/doi/10.1145/321043.321046},
  urldate = {2022-04-24},
  file = {/home/eta/Zotero/storage/VMNS3863/321043.html}
}

@article{magnantiAcceleratingBendersDecomposition1981,
  title = {Accelerating {{Benders Decomposition}}: {{Algorithmic Enhancement}} and {{Model Selection Criteria}}},
  shorttitle = {Accelerating {{Benders Decomposition}}},
  author = {Magnanti, T. L. and Wong, R. T.},
  date = {1981-06},
  journaltitle = {Operations Research},
  volume = {29},
  number = {3},
  pages = {464--484},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  doi = {10.1287/opre.29.3.464},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.29.3.464},
  urldate = {2022-04-24},
  abstract = {This paper proposes methodology for improving the performance of Benders decomposition when applied to mixed integer programs. It introduces a new technique for accelerating the convergence of the algorithm and theory for distinguishing “good” model formulations of a problem that has distinct but equivalent mixed integer programming representations. The acceleration technique is based upon selecting judiciously from the alternate optima of the Benders subproblem to generate strong or pareto-optimal cuts. This methodology also applies to a much broader class of optimization algorithms that includes Dantzig-Wolfe decomposition for linear and nonlinear programs and related “cutting plane” type algorithms that arise in resource directive and price decomposition. When specialized to network location problems, this cut generation technique leads to very efficient algorithms that exploit the underlying structure of these models. In discussing the “proper” formulation of mixed integer programs, we suggest criteria for comparing various mixed integer formulations of a problem and for choosing formulations that can provide stronger cuts for Benders decomposition. From this discussion intimate connections between the previously disparate viewpoints of strong Benders cuts and tight linear programming relaxations of integer programs emerge.}
}

@article{mandiInteriorPointSolving2020,
  title = {Interior {{Point Solving}} for {{LP-based}} Prediction+optimisation},
  author = {Mandi, Jayanta and Guns, Tias},
  date = {2020},
  journaltitle = {NeurIPS},
  abstract = {This work investigates the use of the more principled logarithmic barrier term, as widely used in interior point solvers for linear programming, and considers the homogeneous self-dual formulation of the LP, which shows the relation between the interior point step direction and corresponding gradients needed for learning. Solving optimization problems is the key to decision making in many real-life analytics applications. However, the coefficients of the optimization problems are often uncertain and dependent on external factors, such as future demand or energy or stock prices. Machine learning (ML) models, especially neural networks, are increasingly being used to estimate these coefficients in a data-driven way. Hence, end-to-end predict-and-optimize approaches, which consider how effective the predicted values are to solve the optimization problem, have received increasing attention. In case of integer linear programming problems, a popular approach to overcome their non-differentiabilty is to add a quadratic penalty term to the continuous relaxation, such that results from differentiating over quadratic programs can be used. Instead we investigate the use of the more principled logarithmic barrier term, as widely used in interior point solvers for linear programming. Specifically, instead of differentiating the KKT conditions, we consider the homogeneous self-dual formulation of the LP and we show the relation between the interior point step direction and corresponding gradients needed for learning. Finally our empirical experiments demonstrate our approach performs as good as if not better than the state-of-the-art QPTL (Quadratic Programming task loss) formulation of Wilder et al. and SPO approach of Elmachtoub and Grigas.},
  file = {/home/eta/Zotero/storage/9MSJV4SE/Mandi and Guns - 2020 - Interior Point Solving for LP-based prediction+opt.pdf}
}

@article{mandiSmartPredictOptimize2020,
  title = {Smart  {{Predict-}}­‐and-­‐{{Optimize}}  for  {{Hard}}  {{Combinatorial}}  {{Optimization}}  {{Problems}}},
  author = {Mandi, Jayanta and Demirović, Emir and Stuckey, Peter J. and Guns, Tias},
  date = {2020-06-15},
  journaltitle = {AAAI-20 Technical Tracks 2},
  series = {{{AAAI-20 Technical Tracks}} 2},
  volume = {34},
  pages = {1603--1610},
  publisher = {{AAAI Press}},
  location = {{Palo Alto, California USA}},
  issn = {978-1-57735-835-0},
  url = {http://www.scopus.com/inward/record.url?scp=85099652609&partnerID=8YFLogxK},
  urldate = {2022-02-19},
  abstract = {Combinatorial optimization assumes that all parameters of the optimization problem, e.g. the weights in the objective function, are fixed. Often, these weights are mere estimates and increasingly machine learning techniques are used to for their estimation. Recently, Smart Predict and Optimize (SPO) has been proposed for problems with a linear objective function over the predictions, more specifically linear programming problems. It takes the regret of the predictions on the linear problem into account, by repeatedly solving it during learning. We investigate the use of SPO to solve more realistic discrete optimization problems. The main challenge is the repeated solving of the optimization problem. To this end, we investigate ways to relax the problem as well as warm-starting the learning and the solving. Our results show that even for discrete problems it often suffices to train by solving the relaxation in the SPO loss. Furthermore, this approach outperforms the state-of-the-art approach of wilder2018melding. We experiment with weighted knapsack problems as well as complex scheduling problems, and show for the first time that a predict-and-optimize approach can successfully be used on large-scale combinatorial optimization problems.},
  keywords = {Artificial Intelligence,Optimization and Control}
}

@article{mcdanielModifiedBendersPartitioning1977,
  title = {A {{Modified Benders}}' {{Partitioning Algorithm}} for {{Mixed Integer Programming}}},
  author = {McDaniel, Dale and Devine, Mike},
  date = {1977-11},
  journaltitle = {Management Science},
  volume = {24},
  number = {3},
  pages = {312--319},
  publisher = {{INFORMS}},
  issn = {0025-1909},
  doi = {10.1287/mnsc.24.3.312},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.24.3.312},
  urldate = {2022-04-24},
  abstract = {As applied to mixed-integer programming, Benders' original work made two primary contributions: (1) development of a “pure integer” problem (Problem P) that is equivalent to the original mixed-integer problem, and (2) a relaxation algorithm for solving Problem P that works iteratively on an LP problem and a “pure integer” problem. In this paper a modified algorithm for solving Problem P is proposed, in which the solution of a sequence of integer programs is replaced by the solution of a sequence of linear programs plus some (hopefully few) integer programs. The modified algorithm will still allow for taking advantage of any special structures (e.g., an LP subproblem that is a “network problem”) just as in Benders' original algorithm. The modified Benders' algorithm is explained and limited computational results are given.}
}

@article{millerIntegerProgrammingFormulation1960,
  title = {Integer {{Programming Formulation}} of {{Traveling Salesman Problems}}},
  author = {Miller, C. E. and Tucker, A. W. and Zemlin, R. A.},
  date = {1960-10-01},
  journaltitle = {Journal of the ACM},
  shortjournal = {J. ACM},
  volume = {7},
  number = {4},
  pages = {326--329},
  issn = {0004-5411},
  doi = {10.1145/321043.321046},
  url = {https://doi.org/10.1145/321043.321046},
  urldate = {2022-04-24},
  abstract = {It has been observed by many people that a striking number of quite diverse mathematical problems can be formulated as problems in integer programming, that is, linear programming problems in which some or all of the variables are required to assume integral values. This fact is rendered quite interesting by recent research on such problems, notably by R. E. Gomory [2, 3], which gives promise of yielding efficient computational techniques for their solution. The present paper provides yet another example of the versatility of integer programming as a mathematical modeling device by representing a generalization of the well-known “Travelling Salesman Problem” in integer programming terms. The authors have developed several such models, of which the one presented here is the most efficient in terms of generality, number of variables, and number of constraints. This model is due to the second author [4] and was presented briefly at the Symposium on Combinatorial Problems held at Princeton University, April 1960, sponsored by SIAM and IBM. The problem treated is: (1) A salesman is required to visit each of n cities, indexed by 1, … , n. He leaves from a “base city” indexed by 0, visits each of the n other cities exactly once, and returns to city 0. During his travels he must return to 0 exactly t times, including his final return (here t may be allowed to vary), and he must visit no more than p cities in one tour. (By a tour we mean a succession of visits to cities without stopping at city 0.) It is required to find such an itinerary which minimizes the total distance traveled by the salesman. Note that if t is fixed, then for the problem to have a solution we must have tp ≧ n. For t = 1, p ≧ n, we have the standard traveling salesman problem. Let dij (i ≠ j = 0, 1, … , n) be the distance covered in traveling from city i to city j. The following integer programming problem will be shown to be equivalent to (1): (2) Minimize the linear form ∑0≦i≠j≦n∑ dijxij over the set determined by the relations ∑ni=0i≠j xij = 1 (j = 1, … , n) ∑nj=0j≠i xij = 1 (i = 1, … , n) ui - uj + pxij ≦ p - 1 (1 ≦ i ≠ j ≦ n) where the xij are non-negative integers and the ui (i = 1, …, n) are arbitrary real numbers. (We shall see that it is permissible to restrict the ui to be non-negative integers as well.) If t is fixed it is necessary to add the additional relation: ∑nu=1 xi0 = t Note that the constraints require that xij = 0 or 1, so that a natural correspondence between these two problems exists if the xij are interpreted as follows: The salesman proceeds from city i to city j if and only if xij = 1. Under this correspondence the form to be minimized in (2) is the total distance to be traveled by the salesman in (1), so the burden of proof is to show that the two feasible sets correspond; i.e., a feasible solution to (2) has xij which do define a legitimate itinerary in (1), and, conversely a legitimate itinerary in (1) defines xij, which, together with appropriate ui, satisfy the constraints of (2). Consider a feasible solution to (2). The number of returns to city 0 is given by ∑ni=1 xi0. The constraints of the form ∑ xij = 1, all xij non-negative integers, represent the conditions that each city (other than zero) is visited exactly once. The ui play a role similar to node potentials in a network and the inequalities involving them serve to eliminate tours that do not begin and end at city 0 and tours that visit more than p cities. Consider any xr0r1 = 1 (r1 ≠ 0). There exists a unique r2 such that xr1r2 = 1. Unless r2 = 0, there is a unique r3 with xr2r3 = 1. We proceed in this fashion until some rj = 0. This must happen since the alternative is that at some point we reach an rk = rj, j + 1 {$<$} k. Since none of the r's are zero we have uri - uri + 1 + pxriri + 1 ≦ p - 1 or uri - uri + 1 ≦ - 1. Summing from i = j to k - 1, we have urj - urk = 0 ≦ j + 1 - k, which is a contradiction. Thus all tours include city 0. It remains to observe that no tours is of length greater than p. Suppose such a tour exists, x0r1 , xr1r2 , … , xrprp+1 = 1 with all ri ≠ 0. Then, as before, ur1 - urp+1 ≦ - p or urp+1 - ur1 ≧ p. But we have urp+1 - ur1 + pxrp+1r1 ≦ p - 1 or urp+1 - ur1 ≦ p (1 - xrp+1r1) - 1 ≦ p - 1, which is a contradiction. Conversely, if the xij correspond to a legitimate itinerary, it is clear that the ui can be adjusted so that ui = j if city i is the jth city visited in the tour which includes city i, for we then have ui - uj = - 1 if xij = 1, and always ui - uj ≦ p - 1. The above integer program involves n2 + n constraints (if t is not fixed) in n2 + 2n variables. Since the inequality form of constraint is fundamental for integer programming calculations, one may eliminate 2n variables, say the xi0 and x0j, by means of the equation constraints and produce an equivalent problem with n2 + n inequalities and n2 variables. The currently known integer programming procedures are sufficiently regular in their behavior to cast doubt on the heuristic value of machine experiments with our model. However, it seems appropriate to report the results of the five machine experiments we have conducted so far. The solution procedure used was the all-integer algorithm of R. E. Gomory [3] without the ranking procedure he describes. The first three experiments were simple model verification tests on a four-city standard traveling salesman problem with distance matrix [ 20 23 4 30 7 27 25 5 25 3 21 26 ] The first experiment was with a model, now obsolete, using roughly twice as many constraints and variables as the current model (for this problem, 28 constraints in 21 variables). The machine was halted after 4000 pivot steps had failed to produce a solution. The second experiment used the earlier model with the xi0 and x0j eliminated, resulting in a 28-constraint, 15-variable problem. Here the machine produced the optimal solution in 41 pivot steps. The third experiment used the current formulation with the xi0 and x0j eliminated, yielding 13 constraints and 9 variables. The optimal solution was reached in 7 pivot steps. The fourth and fifth experiments were used on a standard ten-city problem, due to Barachet, solved by Dantzig, Johnson and Fulkerson [1]. The current formulation was used, yielding 91 constraints in 81 variables. The fifth problem differed from the fourth only in that the ordering of the rows was altered to attempt to introduce more favorable pivot choices. In each case the machine was stopped after over 250 pivot steps had failed to produce the solution. In each case the last 100 pivot steps had failed to change the value of the objective function. It seems hopeful that more efficient integer programming procedures now under development will yield a satisfactory algorithmic solution to the traveling salesman problem, when applied to this model. In any case, the model serves to illustrate how problems of this sort may be succinctly formulated in integer programming terms.},
  file = {/home/eta/Zotero/storage/GVSWKM2J/Miller et al. - 1960 - Integer Programming Formulation of Traveling Sales.pdf}
}

@article{niepertImplicitMLEBackpropagating2021,
  title = {Implicit {{MLE}}: {{Backpropagating Through Discrete Exponential Family Distributions}}},
  shorttitle = {Implicit {{MLE}}},
  author = {Niepert, Mathias and Minervini, Pasquale and Franceschi, Luca},
  date = {2021},
  journaltitle = {ArXiv},
  abstract = {Experiments suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations and that it simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Combining discrete probability distributions and combinatorial optimization problems with neural network components has numerous applications but poses several challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a framework for end-to-end learning of models combining discrete exponential family distributions and differentiable neural components. I-MLE is widely applicable as it only requires the ability to compute the most probable states and does not rely on smooth relaxations. The framework encompasses several approaches such as perturbation-based implicit differentiation and recent methods to differentiate through black-box combinatorial solvers. We introduce a novel class of noise distributions for approximating marginals via perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood estimation when used in some recently studied learning settings that involve combinatorial solvers. Experiments on several datasets suggest that I-MLE is competitive with and often outperforms existing approaches which rely on problem-specific relaxations.},
  file = {/home/eta/Zotero/storage/23W9GI37/Niepert et al. - 2021 - Implicit MLE Backpropagating Through Discrete Exp.pdf}
}

@article{papadakosPracticalEnhancementsMagnanti2008,
  title = {Practical Enhancements to the {{Magnanti}}–{{Wong}} Method},
  author = {Papadakos, Nikolaos},
  date = {2008-07-01},
  journaltitle = {Operations Research Letters},
  shortjournal = {Operations Research Letters},
  volume = {36},
  number = {4},
  pages = {444--449},
  issn = {0167-6377},
  doi = {10.1016/j.orl.2008.01.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0167637708000102},
  urldate = {2022-04-24},
  abstract = {The Magnanti–Wong method–accelerating Benders decomposition–is shown to exhibit difficulties due to its dependence on the subproblem; an independent version is therefore introduced. The method additionally requires a–sometimes intractable–master problem core point; for several applications it is proved and experimentally verified that alternative points may be used.},
  langid = {english},
  keywords = {Accelerated Benders decomposition,Benders cut dominance,Magnanti–Wong algorithm,Pareto-optimal cut},
  file = {/home/eta/Zotero/storage/IG5EQEWW/S0167637708000102.html}
}

@inproceedings{pogancicDifferentiationBlackboxCombinatorial2019,
  title = {Differentiation of {{Blackbox Combinatorial Solvers}}},
  author = {Pogančić, Marin Vlastelica and Paulus, Anselm and Musil, Vit and Martius, Georg and Rolinek, Michal},
  date = {2019-09-25},
  url = {https://openreview.net/forum?id=BkevoJSYPB},
  urldate = {2022-02-20},
  abstract = {In this work, we present a method that implements an efficient backward pass through blackbox implementations of combinatorial solvers with linear objective functions.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/eta/Zotero/storage/FR9U4RL3/Pogančić et al. - 2019 - Differentiation of Blackbox Combinatorial Solvers.pdf;/home/eta/Zotero/storage/53EC6BVI/forum.html}
}

@unpublished{powersDifferentiableGreedyNetworks2018,
  title = {Differentiable {{Greedy Networks}}},
  author = {Powers, Thomas and Fakoor, Rasool and Shakeri, Siamak and Sethy, Abhinav and Kainth, Amanjit and Mohamed, Abdel-rahman and Sarikaya, Ruhi},
  date = {2018-10-29},
  eprint = {1810.12464},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.12464},
  urldate = {2022-02-20},
  abstract = {Optimal selection of a subset of items from a given set is a hard problem that requires combinatorial optimization. In this paper, we propose a subset selection algorithm that is trainable with gradient-based methods yet achieves near-optimal performance via submodular optimization. We focus on the task of identifying a relevant set of sentences for claim verification in the context of the FEVER task. Conventional methods for this task look at sentences on their individual merit and thus do not optimize the informativeness of sentences as a set. We show that our proposed method which builds on the idea of unfolding a greedy algorithm into a computational graph allows both interpretability and gradient-based training. The proposed differentiable greedy network (DGN) outperforms discrete optimization algorithms as well as other baseline methods in terms of precision and recall.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/eta/Zotero/storage/FCWCXSIK/Powers et al. - 2018 - Differentiable Greedy Networks.pdf;/home/eta/Zotero/storage/8GG6PK8J/1810.html}
}

@online{RoadNetworkPricing,
  title = {Road Network Pricing and Design for Ordinary and Hazmat Vehicles: {{Integrated}} Model and Specialized Local Search | {{Elsevier Enhanced Reader}}},
  shorttitle = {Road Network Pricing and Design for Ordinary and Hazmat Vehicles},
  doi = {10.1016/j.cor.2019.05.006},
  url = {https://reader.elsevier.com/reader/sd/pii/S0305054819301194?token=9B6D78FA00144B74606898078BB8E2ECC3CF70119A8CC226BA928F361F220C4CDC91BAAEEF2713DDCB9AD698E1D6E22D&originRegion=us-east-1&originCreation=20220327043627},
  urldate = {2022-03-27},
  langid = {english},
  file = {/home/eta/Zotero/storage/QQ88ZT56/Road network pricing and design for ordinary and h.pdf;/home/eta/Zotero/storage/W4SCZL7N/S0305054819301194.html}
}

@inproceedings{sakaueDifferentiableGreedyAlgorithm2021,
  title = {Differentiable {{Greedy Algorithm}} for {{Monotone Submodular Maximization}}: {{Guarantees}}, {{Gradient Estimators}}, and {{Applications}}},
  shorttitle = {Differentiable {{Greedy Algorithm}} for {{Monotone Submodular Maximization}}},
  booktitle = {Proceedings of {{The}} 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Sakaue, Shinsaku},
  date = {2021-03-18},
  pages = {28--36},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v130/sakaue21a.html},
  urldate = {2022-02-20},
  abstract = {Motivated by, e.g., sensitivity analysis and end-to-end learning, the demand for differentiable optimization algorithms has been increasing. This paper presents a theoretically guaranteed differentiable greedy algorithm for monotone submodular function maximization. We smooth the greedy algorithm via randomization, and prove that it almost recovers original approximation guarantees in expectation for the cases of cardinality and κκ\textbackslash kappa-extendible system constraints. We then present how to efficiently compute gradient estimators of any expected output-dependent quantities. We demonstrate the usefulness of our method by instantiating it for various applications.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/home/eta/Zotero/storage/76HZEY3K/Sakaue - 2021 - Differentiable Greedy Algorithm for Monotone Submo.pdf;/home/eta/Zotero/storage/CNK3NC4B/Sakaue - 2021 - Differentiable Greedy Algorithm for Monotone Submo.pdf}
}

@unpublished{tanLearningLinearPrograms2020,
  title = {Learning {{Linear Programs}} from {{Optimal Decisions}}},
  author = {Tan, Yingcong and Terekhov, Daria and Delong, Andrew},
  date = {2020-06-16},
  eprint = {2006.08923},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  url = {http://arxiv.org/abs/2006.08923},
  urldate = {2022-02-19},
  abstract = {We propose a flexible gradient-based framework for learning linear programs from optimal decisions. Linear programs are often specified by hand, using prior knowledge of relevant costs and constraints. In some applications, linear programs must instead be learned from observations of optimal decisions. Learning from optimal decisions is a particularly challenging bi-level problem, and much of the related inverse optimization literature is dedicated to special cases. We tackle the general problem, learning all parameters jointly while allowing flexible parametrizations of costs, constraints, and loss functions. We also address challenges specific to learning linear programs, such as empty feasible regions and non-unique optimal decisions. Experiments show that our method successfully learns synthetic linear programs and minimum-cost multi-commodity flow instances for which previous methods are not directly applicable. We also provide a fast batch-mode PyTorch implementation of the homogeneous interior point algorithm, which supports gradients by implicit differentiation or backpropagation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/eta/Zotero/storage/F4N9C7QU/Tan et al. - 2020 - Learning Linear Programs from Optimal Decisions.pdf;/home/eta/Zotero/storage/QZ3WC8W5/2006.html}
}

@article{tschiatschekDifferentiableSubmodularMaximization2018,
  title = {Differentiable {{Submodular Maximization}}},
  author = {Tschiatschek, Sebastian and Sahin, Aytunc and Krause, Andreas},
  date = {2018},
  pages = {2731--2738},
  url = {https://www.ijcai.org/proceedings/2018/379},
  urldate = {2022-02-20},
  abstract = {Electronic proceedings of IJCAI 2018},
  file = {/home/eta/Zotero/storage/XGN6GW37/379.html}
}

@article{wagnerTwoPhaseMethodSimplex1956,
  title = {A {{Two-Phase Method}} for the {{Simplex Tableau}}},
  author = {Wagner, Harvey M.},
  date = {1956},
  journaltitle = {Operations Research},
  volume = {4},
  number = {4},
  eprint = {167312},
  eprinttype = {jstor},
  pages = {443--447},
  publisher = {{INFORMS}},
  issn = {0030-364X},
  abstract = {A version of a two-phase simplex technique is given for manually solving those linear-programming problems in which artificial vectors are introduced and subsequently driven out. The first phase of the method determines feasibility, provided it exists; the second phase, which follows, searches for optimality.}
}

@report{waitzmanStandardTransmissionIP1990,
  type = {Request for Comments},
  title = {Standard for the Transmission of {{IP}} Datagrams on Avian Carriers},
  author = {Waitzman, David},
  date = {1990-04-01},
  number = {RFC 1149},
  institution = {{Internet Engineering Task Force}},
  doi = {10.17487/RFC1149},
  url = {https://datatracker.ietf.org/doc/rfc1149},
  urldate = {2022-04-16},
  abstract = {This memo describes an experimental method for the encapsulation of IP datagrams in avian carriers. This specification is primarily useful in Metropolitan Area Networks. This is an experimental, not recommended standard.},
  pagetotal = {2},
  file = {/home/eta/Zotero/storage/J6WB8IB4/Waitzman - 1990 - Standard for the transmission of IP datagrams on a.pdf}
}

@unpublished{wangLowrankSemidefiniteProgramming2018,
  title = {Low-Rank Semidefinite Programming for the {{MAX2SAT}} Problem},
  author = {Wang, Po-Wei and Kolter, J. Zico},
  date = {2018-12-15},
  eprint = {1812.06362},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  url = {http://arxiv.org/abs/1812.06362},
  urldate = {2022-02-20},
  abstract = {This paper proposes a new algorithm for solving MAX2SAT problems based on combining search methods with semidefinite programming approaches. Semidefinite programming techniques are well-known as a theoretical tool for approximating maximum satisfiability problems, but their application has traditionally been very limited by their speed and randomized nature. Our approach overcomes this difficult by using a recent approach to low-rank semidefinite programming, specialized to work in an incremental fashion suitable for use in an exact search algorithm. The method can be used both within complete or incomplete solver, and we demonstrate on a variety of problems from recent competitions. Our experiments show that the approach is faster (sometimes by orders of magnitude) than existing state-of-the-art complete and incomplete solvers, representing a substantial advance in search methods specialized for MAX2SAT problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Mathematics - Optimization and Control},
  file = {/home/eta/Zotero/storage/2Q63EIH5/Wang and Kolter - 2018 - Low-rank semidefinite programming for the MAX2SAT .pdf;/home/eta/Zotero/storage/9BMKZQAG/1812.html}
}

@unpublished{wangSATNetBridgingDeep2019,
  title = {{{SATNet}}: {{Bridging}} Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver},
  shorttitle = {{{SATNet}}},
  author = {Wang, Po-Wei and Donti, Priya L. and Wilder, Bryan and Kolter, Zico},
  date = {2019-05-28},
  eprint = {1905.12149},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/1905.12149},
  urldate = {2022-02-19},
  abstract = {Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a "visual Sudok" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/eta/Zotero/storage/BHLGXBQE/Wang et al. - 2019 - SATNet Bridging deep learning and logical reasoni.pdf;/home/eta/Zotero/storage/966Z7SSW/1905.html}
}

@inproceedings{wilderMeldingDataDecisionsPipeline2019,
  title = {Melding the {{Data-Decisions Pipeline}}: {{Decision-Focused Learning}} for {{Combinatorial Optimization}}},
  shorttitle = {Melding the {{Data-Decisions Pipeline}}},
  booktitle = {{{AAAI}}},
  author = {Wilder, B. and Dilkina, B. and Tambe, Milind},
  date = {2019},
  doi = {10.1609/AAAI.V33I01.33011658},
  abstract = {This work focuses on combinatorial optimization problems and introduces a general framework for decision-focused learning, where the machine learning model is directly trained in conjunction with the optimization algorithm to produce highquality decisions, and shows that decisionfocused learning often leads to improved optimization performance compared to traditional methods. Creating impact in real-world settings requires artificial intelligence techniques to span the full pipeline from data, to predictive models, to decisions. These components are typically approached separately: a machine learning model is first trained via a measure of predictive accuracy, and then its predictions are used as input into an optimization algorithm which produces a decision. However, the loss function used to train the model may easily be misaligned with the end goal, which is to make the best decisions possible. Hand-tuning the loss function to align with optimization is a difficult and error-prone process (which is often skipped entirely).We focus on combinatorial optimization problems and introduce a general framework for decision-focused learning, where the machine learning model is directly trained in conjunction with the optimization algorithm to produce highquality decisions. Technically, our contribution is a means of integrating common classes of discrete optimization problems into deep learning or other predictive models, which are typically trained via gradient descent. The main idea is to use a continuous relaxation of the discrete problem to propagate gradients through the optimization procedure. We instantiate this framework for two broad classes of combinatorial problems: linear programs and submodular maximization. Experimental results across a variety of domains show that decisionfocused learning often leads to improved optimization performance compared to traditional methods. We find that standard measures of accuracy are not a reliable proxy for a predictive model’s utility in optimization, and our method’s ability to specify the true goal as the model’s training objective yields substantial dividends across a range of decision problems.},
  file = {/home/eta/Zotero/storage/C8NHAZUW/Wilder et al. - 2019 - Melding the Data-Decisions Pipeline Decision-Focu.pdf}
}

@article{yanSurrogateObjectiveFramework2021,
  title = {A {{Surrogate Objective Framework}} for {{Prediction}}+{{Optimization}} with {{Soft Constraints}}},
  author = {Yan, Kai and Yan, J. and Luo, Chuan and Chen, Liting and Lin, Qingwei and Zhang, Dongmei},
  date = {2021},
  journaltitle = {ArXiv},
  abstract = {A novel analytically differentiable surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints is proposed and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. Prediction+optimization is a common real-world paradigm where we have to predict problem parameters before solving the optimization problem. However, the criteria by which the prediction model is trained are often inconsistent with the goal of the downstream optimization problem. Recently, decision-focused prediction approaches, such as SPO+ and direct optimization, have been proposed to fill this gap. However, they cannot directly handle the soft constraints with the max operator required in many real-world objectives. This paper proposes a novel analytically differentiable surrogate objective framework for real-world linear and semi-definite negative quadratic programming problems with soft linear and non-negative hard constraints. This framework gives the theoretical bounds on constraints’ multipliers, and derives the closed-form solution with respect to predictive parameters and thus gradients for any variable in the problem. We evaluate our method in three applications extended with soft constraints: synthetic linear programming, portfolio optimization, and resource provisioning, demonstrating that our method outperforms traditional two-staged methods and other decision-focused approaches.},
  file = {/home/eta/Zotero/storage/5GTBWZ6A/Yan et al. - 2021 - A Surrogate Objective Framework for Prediction+Opt.pdf}
}


